import math
from typing import Optional

import chex
import jax.numpy as jnp
from scipy.special import gammaln

from ..base import TAction, TLogReward
from ..environment import DAGEnvParams, DAGEnvState


@chex.dataclass
class BaseDAGLikelihood:
    data: chex.Array

    def log_prob(
        self, state: DAGEnvState, env_params: DAGEnvParams
    ) -> TLogReward:
        """Computes the log-likelihood of the data given the state - graph G:

            log P(D | G) = sum_j LocalScore(X_j | Pa_G(X_j))

        Args:
        - state: DAGEnvState, shape [B, ...], batch of states
        - env_params: DAGEnvParams, params of environment,
          always includes reward params

        Returns:
        - TLogReward, shape [B], batch of log-likelihoods
        """
        num_graphs, num_variables = state.adjacency_matrix.shape[:2]
        adjacency_matrix = state.adjacency_matrix.transpose(0, 2, 1)
        parents = adjacency_matrix.reshape(-1, num_variables)
        variables = jnp.tile(jnp.arange(num_variables), num_graphs)

        local_scores = self._local_score(variables, parents)
        local_scores = local_scores.reshape(num_graphs, num_variables)
        return jnp.sum(local_scores, axis=1)  # [B]

    def delta_score(
        self,
        state: DAGEnvState,
        action: TAction,
        next_state: DAGEnvState,
        env_params: DAGEnvParams,
    ) -> TLogReward:
        """Computes the delta-score for adding an edge X_i -> X_j to some grpah
        G, for a specific choice of local score. The delta-score is given by:

            LocalScore(X_j | Pa_G(X_j) U X_i) - LocalScore(X_j | Pa_G(X_j))

        Args:
        - state: DAGEnvState, shape [B, ...], batch of states
        - action: DAGEnvAction, shape [B], batch of actions
        - next_state: DAGEnvState, shape [B, ...], batch of next states
        - env_params: DAGEnvParams, params of environment,
          always includes reward params

        Returns:
        - TLogReward, shape [B], batch of delta-scores
        """
        arange = jnp.arange(state.time.shape[0])  # [B]
        source, target = jnp.divmod(action, env_params.num_variables)
        parents = state.adjacency_matrix[
            arange, :, target
        ]  # [B, num_variables]
        next_parents = next_state.adjacency_matrix[
            arange, :, target
        ]  # [B, num_variables]
        return self._local_score(target, next_parents) - self._local_score(
            target, parents
        )

    def _local_score(
        self,
        variables: chex.Array,
        parents: chex.Array,
    ) -> TLogReward:
        """Computes the local score LocalScore(X_j | Pa_G(X_j)).

        Args:
        - variables: chex.Array, shape [B], batch of variables
        - parents: chex.Array, shape [B, num_variables], batched mask of parents

        Returns:
        - TLogReward, shape [B], batch of local scores
        """
        raise NotImplementedError


class ZeroScore(BaseDAGLikelihood):
    def delta_score(
        self,
        state: DAGEnvState,
        action: TAction,
        next_state: DAGEnvState,
        env_params: DAGEnvParams,
    ) -> TLogReward:
        return jnp.zeros(state.time.shape[0])  # [B]

    def _local_score(
        self, variables: chex.Array, parents: chex.Array
    ) -> TLogReward:
        return jnp.zeros(variables.shape[0])  # [B]


class LinearGaussianScore(BaseDAGLikelihood):
    def __init__(
        self,
        data: chex.Array,
        prior_mean: float = 0.0,
        prior_scale: float = 1.0,
        obs_scale: float = math.sqrt(0.1),
    ):
        super().__init__(data=data)
        self.prior_mean = prior_mean
        self.prior_scale = prior_scale
        self.obs_scale = obs_scale

    def _local_score(
        self, variables: chex.Array, parents: chex.Array
    ) -> TLogReward:
        num_samples, num_variables = self.data.shape
        masked_data = self.data * parents[:, jnp.newaxis]

        means = self.prior_mean * jnp.sum(masked_data, axis=2)
        diffs = (self.data[:, variables].T - means) / self.obs_scale
        y_matrix = self.prior_scale * jnp.matmul(diffs[:, None], masked_data)
        y = jnp.squeeze(y_matrix, axis=1)
        sigma_matrix = (self.obs_scale**2) * jnp.eye(num_variables) + (
            self.prior_scale**2
        ) * jnp.matmul(masked_data.transpose(0, 2, 1), masked_data)
        term1 = jnp.sum(diffs**2, axis=1)
        term2 = -jnp.sum(
            y * jnp.linalg.solve(sigma_matrix, y[..., None])[..., 0], axis=1
        )
        _, term3 = jnp.linalg.slogdet(sigma_matrix)
        term4 = 2 * (num_samples - num_variables) * jnp.log(self.obs_scale)
        term5 = num_samples * jnp.log(2 * jnp.pi)

        return -0.5 * (term1 + term2 - term3 - term4 - term5)


class BGeScore(BaseDAGLikelihood):
    def __init__(
        self,
        data: chex.Array,
        mean_obs: Optional[chex.Array] = None,
        alpha_mu: float = 1.0,
        alpha_w: Optional[float] = None,
    ):
        super().__init__(data=data)
        self.num_samples, self.num_variables = self.data.shape
        if mean_obs is None:
            mean_obs = jnp.zeros((self.num_variables,))
        if alpha_w is None:
            alpha_w = self.num_variables + 2.0

        self.mean_obs = mean_obs
        self.alpha_mu = alpha_mu
        self.alpha_w = alpha_w

        self.t = (self.alpha_mu * (self.alpha_w - self.num_variables - 1)) / (
            self.alpha_mu + 1
        )

        t_matrix = self.t * jnp.eye(self.num_variables)
        data_mean = jnp.mean(data, axis=0, keepdims=True)
        data_centered = data - data_mean

        self.r_matrix = (
            t_matrix
            + jnp.matmul(data_centered.T, data_centered)
            + (
                (self.num_samples * self.alpha_mu)
                / (self.num_samples + self.alpha_mu)
            )
            * jnp.dot((data_mean - self.mean_obs).T, data_mean - self.mean_obs)
        )
        all_parents = jnp.arange(self.num_variables)
        self.log_gamma_term = (
            0.5
            * (
                jnp.log(self.alpha_mu)
                - jnp.log(self.num_samples + self.alpha_mu)
            )
            + gammaln(
                0.5
                * (
                    self.num_samples
                    + self.alpha_w
                    - self.num_variables
                    + all_parents
                    + 1
                )
            )
            - gammaln(
                0.5 * (self.alpha_w - self.num_variables + all_parents + 1)
            )
            - 0.5 * self.num_samples * jnp.log(jnp.pi)
            + 0.5
            * (self.alpha_w - self.num_variables + 2 * all_parents + 1)
            * jnp.log(self.t)
        )

    def _local_score(
        self, variables: chex.Array, parents: chex.Array
    ) -> TLogReward:
        def _logdet(array: chex.Array, mask: chex.Array) -> chex.Array:
            mask = mask[:, None, :] * mask[:, :, None]
            array = mask * array + (1.0 - mask) * jnp.eye(self.num_variables)
            _, logdet = jnp.linalg.slogdet(array)
            return logdet

        num_parents = jnp.sum(parents, axis=1)  # (num_graphs,)
        arange = jnp.arange(parents.shape[0])
        parents_and_variable = parents.at[arange, variables].set(True)

        factor = (
            self.num_samples + self.alpha_w - self.num_variables + num_parents
        )

        log_term_r = 0.5 * factor * _logdet(self.r_matrix, parents) - 0.5 * (
            factor + 1
        ) * _logdet(self.r_matrix, parents_and_variable)

        return self.log_gamma_term[num_parents] + log_term_r
